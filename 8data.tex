\section{Data File}
\subsection{Overview of Data File}
	\begin{enumerate}
		\item Dimensions (years, ages, N fleets, N surveys, etc.)
		\item Fleet and survey names, timing, etc.
		\item Catch amount (biomass or numbers)
		\item Discard
		\item Mean body weight or mean body length
		\item Length composition set-up
		\item Length composition
		\item Age composition set-up
		\item Age imprecision definitions
		\item Age composition
		\item Mean length-at-age or mean bodyweight-at-age
		\item Generalized size composition (e.g. weight frequency)
		\item Tag-recapture
		\item Stock composition (e.g. morphs ID'ed by otolith microchemistry)
		\item Environmental data
		\item Selectivity observations (new placeholder, not yet implemented)
	\end{enumerate}
	
\hypertarget{UnitsOfMeasure}{}
\subsection{Units of Measure}
The normal units of measure are as follows:
\begin{itemize}
	\item Catch biomass -- metric tons	
	\item Body weight -- kilograms	
	\item Body length -- usually in centimeters. Weight at length parameters must correspond to the units of body length and body weight.	
	\item Survey abundance -- any units if catchability (Q) is freely scaled; metric tons or thousands of fish if Q has a quantitative interpretation	
	\item Output biomass -- metric tons	
	\item Numbers -- thousands of fish, because catch is in metric tons and body weight is in kilograms	
	\item Spawning biomass -- metric tons of mature females if eggs/kg = 1 for all weights; otherwise has units that are based on the user-specified fecundity	
\end{itemize}

\hypertarget{RecrTiming}{}
\subsection{Time Units}
	\begin{itemize}
		\item Spawning is restricted to happening once per year at a specified date (in real months).
		\item Recruitment happens at specified recruitment events that occur at user-specified dates (in real months).  \item There can be 1 to many recruitment events; each producing a platoon as a portion of the total recruitment.
		\item A settlement platoon enters the model at age 0 if settlement is between the time of spawning and the end of the year; it enters at age 1 if settlement is after the first of the year; these ages at settlement can be overridden in the settlement setup
        \item All fish advance to the next older integer age on January 1, no matter when they were born during the year.  Consult with your ageing lab to assure consistent interpretation.
		\item Time-varying parameters are allowed to change annually, not seasonally.
		\item Rates like growth and mortality are per year.
	\end{itemize}
	
\subsubsection{Seasons}
	 \begin{itemize}
	 	\item Seasons are the time step during which constant rates apply
	 	\item Catch and discard amounts are per season and F is calculated per season
	 	\item The year can have just 1 annual season, or be subdivided into seasons of unequal length.
	 	\item Season duration is input in real months and is converted into fractions of an annum.  Annual rate values are multiplied by the per annum season duration.
	 	\item If the sum of the input season durations is not close to 12.0, then the input durations is divided by 12.  This allows for a special situation in which the year could be only 0.25 in duration (e.g. seasons as years) so that spawning and time-varying parameters can occur more frequently.	 	
	 \end{itemize}

\subsubsection{Subseasons and Timing of events in SS v.3.30}
\hypertarget{SubSeas}{}
SS v.3.24 and all earlier versions effectively had two subseasons per season because the age-length-key (ALK) for each observation used the mid-season mean length-at-age and spawning occurred at the beginning of a specified season.  Subseasons in SS v.3.30 provide more precision in the timing of events.

	\begin{itemize}
		\item Even number (min = 2) of subseasons per season (regardless of season duration):
			\begin{itemize}
				\item 2 subseasons will mimic SS v.3.24
				\item Specifying more sub seasons will give finer temporal resolution, but will slow the model down, the effect of which is mitigated by only calculating growth as needed.
			\end{itemize}
		\item Survey timing is now cruise-specific and specified in units of months (e.g. April 15 = 4.5).
			\begin{itemize}
				\item sstrans.exe will convert year, season in 3.24 format to year, real month in 3.30 format.
			\end{itemize}
		\item Survey integer season and spawn integer season assigned at runtime based on real month and season duration(s).
		\item The closest subseason is calculated for each observation.
		\item Growth and the age-length-key (ALK) is only calculated at beginning and mid-season or when there is an observation in that subseason.
		\item Fishery body weight uses mid-subseason growth.
		\item Survey body weight and size composition is calculated using the nearest subseason.
		\item Reproductive output now has specified spawn timing (in months fraction) and interpolates growth to that timing.
		\item Survey numbers calculated at cruise survey timing using $e^{-z}$.
		\item Continuous Z for entire season.  Same as applied in version 3.24.
	\end{itemize}

\subsection{Model Dimensions}
\begin{center}
	\begin{tabular}{p{4cm} p{12cm}}
			\textbf{Typical Value} & \textbf{Description} \\
			\hline
			\#V3.30.XX.XX & \multirow{1}{1cm}[-0.1cm]{\parbox{12cm}{Model version number.  This is written by SS in the  new files and a good idea to keep updated in the input files.}} \\
			&  \\
			\hline
			\#C data using new survey & \multirow{1}{1cm}[-0.1cm]{\parbox{12cm}{Data file comment. Must start with \#C to be retained then written to top of various output files.  These comments can occur anywhere in the data file, but must have \#C in columns 1-2.}} \\
			&  \\
			\hline
			1971 & Start year \\
			\hline
			2001 & End year \\
			\hline
			1 & Number of seasons per year \\
			\hline
			12 & \multirow{1}{1cm}[-0.1cm]{\parbox{12cm}{Vector with N months in each season.  These do not need to be integers.  Note:  If the sum of this vector is close to 12.0, then it is rescaled to sum to 1.0 so that season duration is a fraction of a year.  But if the sum is not close to 12.0, then the entered values are simply divided by 12.  So with one season per year and 3 months per season, the calculated season duration will be 0.25, which allows a quarterly model to be run as if quarters are years.  All rates in SS are  calculated by season (growth, mortality, etc.) using annual rates * season duration.}} \\
			& \\
			& \\
			& \\
			& \\
			& \\
			& \\
			& \\
			\hline
			2 & \multirow{1}{1cm}[-0.1cm]{\parbox{12cm}{The number of subseasons.  Entry must be even and the minimum value is 2. This is for the purpose of finer temporal granularity in calculating growth and the associated age-length key.}}\\
			& \\
			& \\
			& \\
			\hline
			\hypertarget{RecrTiminig}{1.5} & \multirow{1}{1cm}[-0.1cm]{\parbox{12cm}{Spawning month; spawning biomass is calculated at this time of year (1.5 means January 15) and used as basis for the total recruitment of all settlement events resulting from this spawning.}}\\
			& \\
			& \\
			\hline
			2 & Number of sexes (1/2) \\
			\hline
			20 & Number of ages. The value here will be the plus-group age.  SS starts at age 0. \\
			\hline
			1 & Number of areas \\
			\hline
			2 & Total number of fishing and survey fleets (which now can be in any order).\\
			\hline
	\end{tabular}
\end{center}


\subsection{Fleet Definitions }
\hypertarget{GenericFleets}{The} catch data input has been modified to improve the user flexibility to add/subtract fishing and survey fleets to a model set-up.  The fleet setup input is transposed so each fleet is now a row.  Previous versions (3.24 and earlier) required that fishing fleets be listed first followed by survey only fleets.  In version 3.30 all fleets now have the same status within the model structure and each has a specified fleet type (except for models that use tag recapture data, this will be corrected in future versions).  Available types are: catch fleet, bycatch only, or survey.  

\begin{center}
	\begin{tabular}{p{2cm} p{2cm} p{2cm} p{2cm} p{2cm} p{4cm} }
		\multicolumn{6}{l}{Inputs that define the fishing and survey fleets:}\\
		\hline
		2 & \multicolumn{5}{l}{\#Number of fleets which includes survey in any order} \\
		\hline
		\#Fleet Type & Timing & Area & Catch Units & Catch Mult. & Fleet Name \\

		\hline
		1 & -1 & 1 & 1 & 0 & FISHERY1\\
		3 &  1 & 1 & 2 & 0 & SURVEY1\\
		\hline
		
	\end{tabular}
\end{center}

\begin{description}
  \item[Fleet Type] \ 
	  \begin{itemize}
	  	\item 1 = fleet with input catches
	  	\item 2 = bycatch fleet (all catch discarded)
	  	\item 3 = survey: assumes no catch removals even if associated catches are specified below.  If you would like to remove survey catch set fleet type to option = 1 with specific month timing for removals (defined below in Timing)
	  	\item 4 = ignored (not yet implemented)
	  \end{itemize}

 \hypertarget{ObsTiming}{}
  \item[Timing]\hfill\\
   Timing for data observations has been revised in v3.30.
	  \begin{itemize}
	  	\item fishery = -1 treat as catch occurred over the whole season or a user can override this assumption by using the code 10XX (e.g 1007 would indicate that catch was removed mid-year in July). Fishery fleets can either have a -1 which means that CPUE and composition observations default to using the total seasonal catch-at-age  and midseason length-at-age, or they can have a timing value of 1 (actually any positive value) in which case the expected value for CPUE and composition observations will be sampled at the time indicated by the month value associated with the observation.  If the -1 code is entered here, then individual observations (e.g., compositional data) can override the midseason default by entering the month as 1000+month.  For example, 1004.5 would be entered for a mid-April observation.
	  	\item survey = 1 The fleet timing here for surveys is not used and only the month value with the observation is relevant (e.g., month specification in the indices of abundance or the month for composition data).  
	  \end{itemize}

	\begin{center}
		\begin{tabular}{| p{2cm}| p{2cm}|  p{2cm}|  p{2cm}|  p{2cm}|  p{2cm}|  }
			\multicolumn{6}{l}{Time steps in SSv3.30 can have finer granularity compared to previous versions}\\
			\multicolumn{6}{l}{where season can be broken into subseason and the age-length key (ALK) can be }\\
			\multicolumn{6}{l}{calculated multiple times over the course of a year:}\\
			\hline
			ALK & ALK* & ALK* & ALK & ALK* & ALK \\
			\hline
			Subseason 1 & Subseason 2 & Subseason 3 & Subseason 4 & Subseason 5 & Subseason 6 \\		
			\hline		
		\end{tabular}
	\end{center}
	
	\begin{itemize}
		\item Continuous Z for entire season;
		\item Even number (min = 2) of subseasons per season (regardless of season duration);
		\item Fishery bodywt uses mid subseason ALK;
		\item SpawnBio has specified spawn\_timing (in months.fraction); uses closest ALK to that timing;
		\item Survey timing is now observation-specific and specified in units of months.fraction (Apr 15 = 4.5);
		\item Survey season and spawn season assigned at runtime based on month and on season duration(s);
		\item Survey body weight and length composition uses closest ALK to survey timing;
		\item ALK* only re-calculated when there is a survey that subseason;
		\item Survey numbers calculated at survey timing using e-Z		
	\end{itemize} 
	  
	  
  \item[Area]\hfill\\
  An integer value indicating the area in which a fleet operates.
  \item[Catch Units] \hfill\\
  Ignored for survey fleets, their units are read later
	  \begin{itemize}
	  	\item 1 = biomass (in metric tons)
	  	\item 2 = numbers (thousands of fish)
	  \end{itemize}   
  See \hyperlink{UnitsOfMeasure}{\textit{Units of Measure}} for more information.
 
  \item[\hypertarget{CatchMult}{Catch Multiplier}] \hfill\\
  Invokes use of a catch multiplier, which is then entered as a parameter in the MG parameter section.  The estimated value or fixed value of the catch multiplier is multiplied by the estimated catch before being compared to the observed catch. 
	  \begin{itemize}
	  	\item 0 = No catch multiplier used.
	  	\item 1 = Apply a catch multiplier which is defined as an estimable parameter in the control file after the cohort growth deviation in the biology parameter section. The model’s estimated retained catch will be multiplied by this factor before being compared to the observed retained catch.
	  \end{itemize}  
\end{description}

\subsection{Optional Bycatch Fleets}
The option to include bycatch fleets was introduced in Stock Synthesis version 3.30.10.  This is an optional input and if no bycatch is to be included in the catches this section can be ignored.\\

\noindent If a fleet above was set as a bycatch fleet (fleet type = 2), the following line is required: 
\begin{center}
	\begin{tabular}{p{2cm} p{2cm} p{2cm} p{2cm} p{2cm} p{4cm} }

		\multicolumn{6}{l}{Optional inputs that define bycatch fleet:}\\
		\hline
		\#Fleet Index & Include in MSY & Fmult & F or First Year & F or Last Year & Not used \\		
		\hline
		1 & 1  & 1 & 1982 & 2010 & 999\\
		\hline		
	\end{tabular}
\end{center}

\begin{description}
  \item[Fleet Index] \
	  \begin{itemize}
	  	\item Fleet to include bycatch catch for.  The fleet type above in the fleet definition should be fleet type =2.  If there are multiple bycatch fleets, then a line for each fleet is required in the bycatch section.
	  \end{itemize}
	  
  \item[Include in MSY] \
  \begin{itemize}
  	\item 1 = deadfish in MSY, ABC, and other benchmark and forecast output
  	\item 2 = omit from MSY and ABC (but still include the mortality)
  \end{itemize}
  
  \item[Fmult] \
  \begin{itemize}
  	\item 1 = F multiplier scales with other fleets
  	\item 2 = Bycatch F constant at input value
  	\item 3 = Bycatch F from range of years
  \end{itemize}
  
  \item[F First Year] \
  \begin{itemize}
  	\item F or first year of range
  \end{itemize}

  \item[Last Year] \
  \begin{itemize}
  	\item Last year of range
  \end{itemize}
  
  \item[Not Used] \
  \begin{itemize}
  	\item This column is not yet used and is reserved for future features.
  \end{itemize}

\end{description}

\subsection{Catch}
\hypertarget{CatchFormat}{After} reading the fleet-specific indicators, a list of catch values by fleet and season are read in by the model.  The format for the catches is year, season that the catch will be attributed to, fleet, a catch value, and a year specific catch standard error.   Only positive catches need to be entered, so there is no need for records for the survey fleets.  To include an equilibrium catch value the year should be noted as -999 and this is now season specific .  \hypertarget{ListBased}{There} is no longer a need to specify the number of records to be read; instead the list is terminated by entering a record with the value of -9999 in the year field. The updated list based approach extends throughout the data file (e.g. catch, length- and age-composition data), the control file (e.g. lambdas), and the forecast file (e.g. total catch by fleet, total catch by area, allocation groups, forecasted catch).

In addition, it is possible to collapse the number of seasons.  So if a season value is greater than the N seasons for a particular model, that catch is added to the catch for N seasons.  This is generally to collapse a seasonal model into an annual model.  In a seasonal model, use of season=0 will cause SS to distribute the input value of catch equally among the N seasons.  SS assumes that catch occurs continuously over seasons and hence is not specified as month in the catch data section.  However, all other data types will need to be specified by month.

The new format for version 3.30 for a 2 season model with 2 fisheries looks like the table below.  The example is sorted by fleet, but the sort order does not matter.  In data.ss\_new, the sort order is fleet, year, season.

\begin{center}
	\begin{tabular}{p{3cm} p{3cm} p{3cm} p{3cm} p{2cm}}
		\multicolumn{5}{l}{\#Catches by year, season for every fleet:}\\
		\hline
		\# Year & Season & Fleet & Catch & Catch SE \\
		\hline
		-999 & 1 & 1 & 56  & 0.05 \\
		-999 & 2 & 1 & 62  & 0.05 \\
		1975 & 1 & 1 & 876 & 0.05 \\
		1975 & 2 & 1 & 343 & 0.05 \\
		...  & ...   & ...   & ...   & ...  \\
		-999 & 1 & 2 & 55  & 0.05 \\
		-999 & 2 & 2 & 22  & 0.05 \\
		1975 & 1 & 2 & 555 & 0.05 \\
		1975 & 2 & 2 & 873 & 0.05 \\
		...  & ...   & ...   & ...   & ...  \\
		-9999 & 0 & 0 & 0 & 0 \\
		\hline
	\end{tabular}
\end{center}

\begin{itemize}
	\item Catch can be in terms of biomass or numbers for each fleet.
	\item Catch is retained catch. If there is discard also, then it is handled in the discard section below.  This is the recommended setup which results in a model estimated retention curve based upon the discard data (specifically discard composition data).  However, there may be instances where the data do not support estimation of retention curves.  In these instances catches can be specified as all dead (retained + discard estimates).
	\item If there is reason to believe that the retained catch values underestimate the true catch, then it is possible in the retention parameter set up to create the ability for the model to estimate the degree of unrecorded catch.  However, this is better handled with the new catch multiplier option.
\end{itemize}
\subsubsection{Bycatch}
Bycatch fleets have an F so impose mortality and catch fish.  All this catch is discarded.  There must be a value entered for retained catch so that SS knows to calculate an F for that season, but this catch amount is ignored in the log likelihood.  The amount of discarded catch can be entered as a discard observation(s).  Bycatch fleets have selectivity, which must be specified or estimated if observations of the size or age composition of the discards is entered.
\begin{itemize}
	\item Because there is no retained catch amount to match, the F for bycatch only fleets must be by th continuous F method  (F\_method = 2).
	\item  MSY and yield per recruit are calculated in terms of dead catch, and they currently include catch from bycatch fleets.  So the search for Fmsy scales the bycatch F along with the F for the fleets that retain catch.  A future version of SS3.30 will implement more controls for how bycatch fleets are handled in MSY and in forecast calculations. 
\end{itemize}

\subsection{Indices}
Indices are data that are compared to aggregate quantities in the model.  Typically the index is a measure of fish abundance, but this data section also allows for the index to be related to a fishing fleet's F, or to another quantity estimated by the model.  The first section of the Indices setup contains the fleet ID, Units, and error distribution for each fleet that has index data.

\begin{center}
	\begin{tabular}{p{3cm} p{2cm} p{3cm} p{6cm}}
		\multicolumn{4}{l}{\#CPUE and Suvey Abundance Observations:}\\
		\hline
		\#Fleet/Survey & Units & \#Error      &  \\
		               &       & Distribution & SD\_Report \\
		\hline
		1 & 1 & 0 & 0 \\
		2 & 1 & 0 & 0 \\
		... & ... & ... & ... \\
		\hline
	\end{tabular}		
\end{center}

\begin{description}
	\item[\hypertarget{IndexUnits}{Units}]\hfill\\	
	\begin{itemize}
		\item 0  = numbers
		\item 1  = biomass
		\item 2  = F
		\begin{itemize}
			\item 	Note the “F” option can only be used for a fishing fleet and not for a survey, even if the survey selectivity is mirrored to a fishing fleet.  The values of these effort data are interpreted as proportional to the level of the fishery F values.  No adjustment is made for differentiating between continuous F values versus exploitation rate values coming from Pope’s approximation.  A normal error structure is recommended so that the input effort data are compared directly to the model’s calculated F, rather than to loge(F).  The resultant proportionality constant has units of 1/Q. The options for units are:	
		\end{itemize}
		\item >=30 special survey types.  These options bypass the calculation of survey selectivity so no selectivity parameters should be entered and especially not estimated.  The expected values for these types are:
		\begin{itemize}
			\item 30 = spawning biomass (e.g. for an egg and larvae survey)
			\item 31 = exp(recruitment deviation), useful for environmental index affecting recruitment
			\item 32 = spawning biomass * exp(recruitment deviation), for a pre-recruit survey occurring before density-dependence
			\item 33 = recruitment, age-0 recruits
			\item 34 = depletion (spawning biomass/virgin spawning biomass)
			\item 35 = survey of a deviation vector ($e(survey(y)) = f(parm\_dev(k,y))$), can be used for an environmental time-series with soft linkage to the index.
		\end{itemize}
	    For survey type 35, the selected deviation vector is specified in Q section of the control file. 
	     
	    \setlength{\parindent}{25pt}
	    Special survey option 34 automatically adjusts phases of parameters.  There are options for additional control over this in the control file Q setup section under the link information column where:
	    \begin{itemize}
		    \item 0 = add 1 to phases of all parameters; only R0 active in new phase 1; mimics the default option of previous model versions
		    \item 1 = only R0 active in phase 1; then finish with no other parameters becoming active; useful for data-limited draws of other fixed parameters.  Essentially, this option allows SS to mimic DB-SRA. 
		    \item 2 = no phase adjustments, can be used when profiling on fixed R0
	    \end{itemize}
	\end{itemize}
	
	\item[Error Distribution]\
	\begin{itemize}
		\item -1 = normal error
		\item  0 = lognormal error 
		\item >0 = Student's t-distribution in log space with degrees of freedom equal to this value.  For DF>30, results will be nearly identical to that for lognormal distribution.  A DF value of about 4 gives a fat-tail to the distribution (see Chen (2003)).  The se values entered in the data file must be the standard error in log\textsubscript{e} space.
	\end{itemize}

Abundance indices typically have a lognormal error structure with units of standard error of log\textsubscript{e}(index).  If the variance of the observations is available only as a CV, then the value of se can be approximated as $\sqrt{(log\textsubscript{e}(1+(CV)\textsuperscript{2}))}$ where CV is the standard error of the observation divided by the mean value of the observation.

For the normal error structure, the entered values for se are interpreted directly as a se in arithmetic space and not as a CV.  Thus switching from a lognormal to a normal error structure forces the user to provide different values for the se input in the data file.

If the data exist as a set of normalized Z-scores, you can either:  assert a lognormal error structure after entering the data as exp(Z-score) because it will be logged by SS.  Preferably, the Z-scores would be entered directly and the normal error structure would be used.

	\item[Enable SD\_Report]\
	
	Indices with SD\_Report enabled will have the expected values for their historical values appear in the ss.std and ss.cor files. The default value is for this option is 0.
	\begin{itemize}
		\item 0 = SD\_Report not enabled for this index
		\item 1 = SD\_Report enabled for this index
	\end{itemize}

\end{description}

\begin{description}
	\item[Data Format]\
\begin{center}
	\begin{tabular}{p{3cm} p{2cm} p{3cm} p{3cm} p{2.5cm}}
		\hline
		\#Year & Month & Fleet/Survey & Observation & SE \\
		\hline
		1991 & 7   & 3   & 80000 & 0.056 \\
		1995 & 7.2 & 3   & 65000 & 0.056 \\
		...  & ... & ... & ...   & ... \\
		2000 & 7.1 & 3   & 42000 & 0.056 \\
		-9999 & 1  & 1   & 1     & 1 \\ 
		\hline
	\end{tabular}
\end{center}
	\begin{itemize}
		\item For fishing fleets, catch-per-unit-effor (CPUE) is defined in terms of retained catch (biomass or numbers).
		\item For fishery independent surveys, retention/discard is not defined so CPUE is implicitly in terms of total CPUE.
		\item If a survey has its selectivity mirrored to that of a fishery, only the selectivity is mirrored so the expected CPUE for this mirrored survey does not use the retention curve (if any) for the fishing fleet.
		\item If the fishery or survey has time-varying selectivity, then this changing selectivity will be taken into account when calculating expected values for the CPUE or survey index.
		\item Year values that are before start year or after end year are excluded from model, so the easiest way to include provisional data in a data file is to put a negative sign on its year value.
		\item Duplicate survey observations are not allowed.
		\item Observations can be entered in any order, except if the super-year feature is used.
		\item Observations that are to be included in the model but not included in the negative log likelihood need to have a negative sign on their fleet ID.  Previously the code for not using observations was to enter the observation itself as a negative value.  However, that old approach prevented use of a Z-score environmental index as a “survey”.  This approach is best for single or select years from an index rather than an approach to remove a whole index.  Removing a whole index from the model should be done through the use of lambdas at the bottom of the control file which will eliminate the index from model fitting. 
		\item Super-periods are turned on and then turned back off again by putting a negative sign on the season.  Previously, super-periods were started and stopped by entering -9999 and the -9998 in the SE field.  See the “Data Super-Period” section of this manual for more information.
		\item RESEARCH NOTE:  If the statistical analysis used to create the CPUE index of a fishery has been conducted in such a way that its inherent size/age selectivity differs from the size/age selectivity estimated from the fishery’s size and age composition, then you may want to enter the CPUE as if it was a separate survey and with a selectivity that differs from the fishery’s estimated selectivity.  The need for this split arises because the fishery size and age composition should be derived through a catch-weighted approach (to appropriately represent the removals by the fishery) and the CPUE should be derived through an area-weighted approach to better serve as a survey of stock abundance.
	\end{itemize}
\end{description}

\subsection{Discard}
If discard is not a feature of the model specification, then just a single input is needed:

\begin{center}
\begin{tabular}{p{2cm} p{13cm}}
	\#Input & Description\\
	\hline
	0 & \#Number of fleets with discard observations\\
	\hline
\end{tabular}
\end{center}

	
\noindent	
If discard is being used, the input syntax is:
	\begin{center}
		\begin{tabular}{p{2cm} p{3cm} p{3cm} p{3cm} p{3cm}}
			\#Input & \multicolumn{4}{l}{Description}\\
			\hline
			1 & \multicolumn{4}{l}{\#Number of fleets with discard observations}\\
			\hline
			\#Fleet & Units & \multicolumn{3}{l}{Error Distribution}\\
			\hline
			1 & 2 & \multicolumn{3}{l}{-1}\\
			\hline
			\#Year & Month & Fleet  & Observation & Standard Error \\
			\hline
			1980  & 7 & 1 & 0.05 & 0.25 \\
			1991  & 7 & 1 & 0.10 & 0.25 \\
			-9999 & 1 & 1 & 0    & 0 \\
			\hline
		\end{tabular}
	\end{center}
	
\begin{description}
	\item[Discard Units]\ 
	\begin{itemize}
		\item 1 = values are amount of discard in either biomass or numbers according to the selection made for retained catch
		\item 2 = values are fraction (in biomass or numbers) of total catch discarded; bio/num selection matches that of retained catch
		\item 3 = values are in numbers (thousands) of fish discarded, even if retained catch has units of biomass
	\end{itemize}
	\item[Discard Error Distribution]\hfill\\
	The four options for discard error are:
	\begin{itemize}
		\item >0 = degrees of freedom for Student's t-distribution used to scale mean body weight deviations.  Value of error in data file in interpreted as CV of the observation.
		\item 0 = normal distribution, value of error in data file is interpreted as CV of the observation
		\item -1 = normal distribution, value of error in data file is interpreted as standard error of the observation
		\item -2 = lognormal distribution, value of error in data file is interpreted as standard error of the observation in log space 
		\item -3 = truncate normal distribution (new with 3.30, needs further testing), value of error in data file is interpreted as standard error of the observation.  This is a good option for low observed discard rates.

	\end{itemize}
	\item[Discard notes]\
	\begin{itemize}
		\item Since discard refers to catch, its time units are in seasons, not months.
		\item Year values that are before start year or after end year are excluded from model, so the easiest way to include provisional data in a data file is to put a negative sign on its year value.
		\item Negative value for fleet causes it to be included in the calculation of expected values, but excluded from the log likelihood.
		\item Zero (0.0) is a legitimate discard observation, unless lognormal error structure is used.
		\item Duplicate discard observations are not allowed.
		\item Observations can be entered in any order, except if the super-period feature is used. 
		\item Note that in the control file you will enter information for retention such that 1-retention is the amount discarded.  All discard is assumed dead, unless you enter information for discard mortality.  Retention and discard mortality can be either size-based of (new with 3.30) age-based.
	\end{itemize}
	\item[Cautionary Note]\hfill\\
	The use of CV as the measure of variance can cause a small discard value to appear to be overly precise, even with the minimum standard error of the discard observation set to 0.001.  In the control file, there is an option to add an extra amount of variance.  This amount is added to the standard error, not to the CV, to help correct this problem of underestimated variance.
\end{description}

\subsection{Mean Body Weight or Length}
This is the overall mean body weight or length across all selected sizes and ages.  This may be useful in situations where individual fish are not measured but mean weight is obtained by counting the number of fish in a specified sample, e.g. a 25 kg basket.  Observations can be  in terms of mean length by setting switching the partition code to a negative value (e.g. -0, -1, -2) rather than 0, 1, and 2 typically used with the mean body weight approach.

\begin{center}
	\begin{tabular}{p{2cm} p{2cm} p{2cm} p{2cm} p{2cm} p{2cm}}
		\multicolumn{6}{l}{\#Mean Body Weight Data Section}\\
		\hline
		%\\
		1  & \multicolumn{5}{l}{Use mean body size data (0/1) } \\
		\hline
		30 & \multicolumn{5}{l}{\#Degrees of freedom for Student's t-distribution used to evaluate mean } \\
		   & \multicolumn{5}{l}{\parbox{13cm }{ body weight deviation.  This is not a conditional input, must be here even if there are no mean body weight observations.}}\\
		\hline
		\#Year & Month & Fleet & Partition & Observation & Standard Error \\
		\hline
		1990  & 7 & 1 & 0 & 4.0 & 0.95 \\
		1990  & 7 & 1 & 0 & 1.0 & 0.95 \\
		-9999 & 0 & 0 & 0 & 0   & 0 \\
		\hline
	\end{tabular}
\end{center}

\begin{description}
	\item[Partition]\hfill\\
	Mean weight data and composition data require specification of what group the sample originated from (e.g. discard, retained, discard + retained).
	\begin{itemize}
		\item 0 = whole catch in units of weight (discard + retained)
		\item 1 = discarded catch in units of weight
		\item 2 = retained catch in units of weight
		\item -0 = whole catch in units of length (discard + retained)
		\item -1 = discarded catch in units of length
		\item -2 = retained catch in units of length
		%\item 10 = whole catch in units of length (discard + retained)
		%\item 11 = discarded catch in units of length
		%\item 12 = retained catch in units of length
	\end{itemize}
	\item[Observation - Units]\hfill\\
	Units must correspond to the units of body weight, normally in kilograms, (or mean length in cm).  The expected value of mean body weight (or mean length) is calculated in a way that incorporates effect of selectivity and retention.
	\item[Error]\hfill\\
	Error is entered as the CV of the observed mean body weight (or mean length)
\end{description}

\subsection{Population Length Bins}
The first part of the length composition section sets up the bin structure for the population.  These bins define the granularity of the age-length key and the coarseness of the length selectivity.  Fine bins create smoother distributions, but a larger and slower running model.
First read a single value to select one of three population length bin methods, then any conditional input for options 2 and 3:

\begin{center}

		\begin{tabular}{p{2cm} p{5cm} p{8cm}}
		\hline
		1 & \multicolumn{2}{l}{use data bins to be read later.  No additional input here} \\
		\hline
		2 & \multicolumn{2}{l}{generate from bin width min max, read next:} \\
		\multirow{4}{2cm}[-0.1cm]{} & 2 & Bin width \\
								    & 10 & Lower size of first bin\\
									& 82 & Lower size of largest bin\\
		\multicolumn{3}{l}{The number of bins is then calculated from: (max Lread - min Lread)/(bin width) + 1}\\
		\hline
		3 & \multicolumn{2}{l}{Read 1 value for number of bins, and then read vector of bin boundaries} \\
		\multirow{2}{2cm}[-0.1cm]{} & 25 & Number of population length bins to be read\\ 
									& 26 28 30 ... & Vector containing lower edge of each population size bin \\
		%\hline
		%\multicolumn{3}{l}{End of conditional inputs for length bin method.}\\
		\hline									  
	\end{tabular}
	
\end{center}
\begin{description}
	\item[Notes:]\
	\begin{itemize}
		\item For option 2, bin width should be a factor of min size and max size.  For options 2 and 3, the population length bins must not be wider than the length data bins, but the boundaries of the bins do not have to align.  The transition matrix between population and data length bins is output to echoinput.sso.
		\item The mean size at settlement (virtual recruitment age) is set equal to the min size of the first population length bin.
		\item When using more, finer population length bins, SS will create smoother length selectivity curves and smoother length distributions in the age-length key, but run slower (more calculations to do).
		\item The mean weight-at-length, maturity-at-length and size-selectivity are based on the mid-length of the population bins.  So these quantities will be rougher approximations if broad bins are defined.
		\item Provide a wide enough range of population size bins so that the mean body weight-at-age will be calculated correctly for the youngest and oldest fish.  If the growth curve extends beyond the largest size bin, then these fish will be assigned a length equal to the mid-bin size for the purpose of calculating their body weight.
		\item While exploring the performance of models with finer bin structure, a potentially pathological situation has been identified.  When the bin structure is coarse (note that some applications have used 10 cm bin widths for the largest fish), it is possible for a selectivity slope parameter or a retention parameter to become so steep that all of the action occurs within the range of a single size bin.  In this case, the model will see zero gradient of the log likelihood with respect to that parameter and convergence will be hampered.
		\item ALK Tolerance:  A value read near the end of the starter.ss file defines the degree of tail compression used for age-length key.  If this is set to 0.0, then no compression is used and all cells of the age-length key are processed , even though they may contain trivial (e.g. 1 e-13) fraction of the fish at a given age.  With tail compression of, say 0.0001, SS will at the beginning of each phase calculate the min and max length bin to process for each age of each morphs ALK and compress accordingly.  Depending on how many extra bins are outside this range, you may see speed increases near 10-20\%.  Large values of ALK tolerance, say 0.1, will create a sharp end to each distribution and likely will impede convergence.  Try out ALK tolerance.
	\end{itemize}
\end{description}

\begin{description}
	\item[Length Composition Data Structure]\
		{\\Enter a code to indicate whether or not length composition data will be used.}
\end{description}

\begin{tabular}{p{2cm} p{13cm}}
		%\hline
		%\multicolumn{3}{l}{End of conditional inputs for length bin method.}\\
		\hline	
		1 & Use length composition data (0/1)\\
		\hline									  
\end{tabular}
\\\\If the value 0 is entered, then skip all length related inputs below and skip to the age data setup section.  Otherwise continue:\\
	

\begin{tabular}{p{2cm} p{2cm} p{2cm} p{1.75cm} p{1cm} p{2cm} p{2cm} p{2cm}}
		\multicolumn{7}{l}{Specify bin compression and error structure for length composition data for each fleet:}\\
		\hline
		\#Min Tail Compression & Constant added to proportions & Combine males \& females & Compress Bins & \hypertarget{Dirichlet}{Comp Error Dist.} & Dirichlet Parameter Select & Min Sample Size\\
		\hline
		0 & 0.0001 & 0 & 0 & 0 & 0 & 1 \\
		0 & 0.0001 & 0 & 0 & 0 & 0 & 1 \\
		\hline
\end{tabular}


\begin{description}
	\item[Minimum Tail Compression]\hfill\\
	Compress tails of composition until observed proportion is greater than this value; negative value causes no compression; Advise using no compression if data are very sparse, and especially if the set-up is using agecomp within length bins because of the sparseness of these data.

	\item[Added Constant]\hfill\\
	Constant added to observed and expected proportions at length and age to make logL calculations more robust.  Tail compression occurs before adding this constant.  Proportions are renormalized to sum to 1.0 after constant is added.

	\item[Combine Males \& Females]\hfill\\
	Combine males into females at or below this bin number.  This is useful if the sex determination of very small fish is doubtful so allows the small fish to be treated as combined sex.  If CombM+F>0, then add males into females for bins 1 thru this number, zero out the males, set male data to start at the first bin above this bin.  Note that CombM+F is entered as a bin index, not as the size associated with that bin.  Comparable option is available for age composition data.

	\item[Compress Bins]\hfill\\
	This options allows for the compression of length or age bins beyond a specific length or age by each data source.  As an example,  a value of 5 in the compress bins column would condense the final five length bins for the specified data source.
	
	\item[Composition Error Distribution]\
	\begin{itemize}
		\item 0 = Multinomial Error
		\item 1 = Dirichlet Multinomial Error
		\begin{itemize}
			\item The Dirichlet Multinomial Error distribution requires the addition of a parameter lines for the natural log of the effective sample size multiplier ($\theta$) at the end of the selectivity parameter section in the control file. \hyperlink{Dirichletparameter}{\textit{Click here to see the parameter setup in the control file.}}
			\item The Parameter Select option needs be used to specified which data sources should be weighted together or separate. 
		\end{itemize}
	\end{itemize}
	
	\item[Parameter Select]\
		\begin{itemize}
			\item 0 = Default
			\item 1-N = Only used for the Dirichlet option. Set to a sequence of numbers from 1 to N where N is the total number of combinations of fleet and age/length. That is, if you have 3 fleets with length data, but only 2 also have age data, you would have values 1 to 3 in the length comp setup and 4 to 5 in the age comp setup. You can also have a data weight that is shared across fleets by repeating values in Parameter Select.
		\end{itemize}	

	\item[Minimum Sample Size]\hfill\\
	The minimum value (floor) for all sample sizes. This value must be at least 0.001. Conditional age-at-length data may have observations with sample sizes less than 1. Stock Synthesis 3.24 has an implicit minimum sample size value of 1.
\end{description}

\subsection{Dirichlet Parameter Number and Effective Sample Sizes}
If the Dirichlet multinomial error distribution is selected, indicate here which of a list of Dirichlet multinomial parameters will be used for this fleet.  So each fleet could use a unique Dirichlet multinomial parameter, or all could share the same, or any combination of unique and shared.  The requested number of Dirichlet multinomial parameters will be read from the control file. Please note that age-compositions Dirichlet multinomial parameters are continued after length-compositions, so a model with one fleet and both data types would presumably require two new Dirichlet multinomial parameters.  	
	
The Dirichlet estimates the effective sample size as $N_{eff}=\frac{1}{1+\theta}+\frac{N\theta}{1+\theta}$ where $\theta$ is the estimated parameter and $N$ is the input sample size.  Stock Synthesis estimates the log of the Dirichlet multinomial parameter such that $\hat{\theta}_{\text{fishery}} = e^{-0.6072} = 0.54$ where assuming $N=100$ for the fishery would result in an effective sample size equal to 35.7.
	
This formula for effective sample size implies that, as the Stock Synthesis parameter log\_Theta goes to large values (i.e., 20), then  $N_{eff}$ will converge to the input sample size ($N_{input}$).  In this case, small changes in the value of the log\_Theta parameter has no action, and the derivative of the negative log-likelihood is zero with respect to the parameter, which means the Hessian will be singular and cannot be inverted. To avoid this non-invertible Hessian when the log\_Theta parameter becomes large, turn it off while fixing it at the high value. In summary, we recommend setting the upper bound for the Dirichlet multinomial parameter log\_Theta to a high value (i.e., 20-25), and then if any fleet has an estimate of log\_Theta $>$ 15 then turn that Dirichlet multinomial parameter off while starting it at the estimated high value.  This is equivalent to turning off down-weighting of fleets where evidence suggests that $N_{eff}$ = $N_{input}$.  
	
For additional information about the Dirichlet multinomial please see Thorson et al. 2017. \textit{Model-based estimates of effective sample size in stock assessment models using the Dirichlet-multinomial distribution}. Fisheries Research, 192: 84-93.

\hypertarget{CompTiming}{}
\subsection{Length Composition Data}
\begin{center}
	\begin{tabular}{p{1.5cm} p{1.5cm} p{1.5cm} p{1.5cm} p{1.5cm} p{1.5cm} p{5cm}}
		\hline
		\multicolumn{2}{l}{30} & \multicolumn{5}{l}{\#Number of length bins for data}\\
		\hline
		\multicolumn{2}{l}{26 28 30 ... 88 90} &  \multicolumn{5}{l}{\#Vector of length bins associated with the length data}\\
		\hline
		\\
		\multicolumn{7}{l}{Example of a single length composition observation:} \\
		\hline
		\#Year & Month & Fleet & Sex & Partition & Nsamp & data vector\\
		\hline
		1986 & 1 & 1 & 3 & 0 & 20 & <female then male data> \\
		... & ...& ... & ... & ...& ... & ... \\
		-9999 & 0 & 0 & 0 & 0 & 0 & 0 \\
		\hline	
	\end{tabular}
\end{center}

\begin{description}
	\item[Sex]\hfill\\
	If model has only one sex defined in the set-up, all observations must have sex set equal to 0 or 1.  In a 2 sex model, the data vector always has female data followed by male data, even if only one of the two sexes has data that will be used.
	\begin{itemize}
		\item Sex = 0 means combined male and female (must already be combined and information placed in the female portion of the data vector) (male entries must exist for correct data reading, then will be ignored).
		\item Sex = 1 means female only (male entries must exist for correct data reading, then will be ignored).
		\item Sex = 2 means male only (female entries must exist and will be ignored after being read).
		\item Sex = 3 means data from both sexes will be used and they are scaled so that they together sum to 1.0; i.e. sex ratio is preserved.
	\end{itemize}
	\item[Partition]\hfill\\
	Partition indicates samples from either discards,retained, or combined.
	\begin{itemize}
		\item 0 = combined
		\item 1 = discard
		\item 2 = retained
	\end{itemize}
	\item[Excluding Data]\
	\begin{itemize}
		\item If the value of year is negative, then that observation is not transferred into the working array.  This feature is the easiest way to include observations in a data file but not to use them in a particular model scenario.
		\item If the value of fleet is negative, then the observation is processed and its expected value and log likelihood is calculated, but this log likelihood is not included in the total log likelihood.  This feature allows the user to see the fit to a provisional observation without having that observation affect the model.
	\end{itemize}
	\item[Note:]\
	\begin{itemize}
		\item Version 3.30 no longer requires that the number of length composition data lines be specified.  Entering -9999 at the end of the data matrix will indicate to the model the end of length composition lines to be read.
		\item Each observation can be stored as one row for ease of data management in a spreadsheet and for sorting of the observations.  However, the 6 header values, the female vector and the male vector could each be on a separate line because ADMB reads values consecutively from the input file and will move to the next line as necessary to read additional values.
		\item The composition observations can be in any order and replicate observations are allowed (unlike survey and discard data).  However, if the super-period approach is used, then each super-periods’ observations must be contiguous in the data file.
	\end{itemize}
\end{description}

\subsection{Age Composition Option}
The age composition section begins by reading the number of age bins.  If the value 0 is entered for the number of age bins, then SS skips reading the bin structure and all reading of other age composition data inputs.
\begin{center}
	\begin{tabular}{p{3cm} p{12cm}  }
		\hline
		17 & \#Number of age' bins; can be equal to 0 if age data not used; do not include a vector of agebins if Nage' bins is set equal to 0.\\
		\hline
	\end{tabular}
\end{center}


\subsubsection{Age Composition Bins}
If a positive number of age bins is read, then SS reads the bin definition next.
\begin{center}
	\begin{tabular}{p{3cm} p{12cm}  }
		\hline
		1 2 3 ... 20 25 & \# Vector of ages\\
		\hline		
	\end{tabular}
\end{center}
The bins are in terms of observed age (here age’) and entered as the lower edge of each bin.
\\Each ageing imprecision definition is used to create a matrix that translates true age structure into age' structure.
\\The first and last age' bins work as accumulators.  So in the example any age 0 fish that are caught would be assigned to the age’=1 bin.

\subsubsection{Ageing Error}
Here, the capability to create a distribution of age’ (e.g. age with possible bias and imprecision) from true age is created.  One or many age error definitions can be created.  For each, there is input of a vector of mean age’ and stddev of age’.  For one definition, the input vectors can be replaced by vectors created from estimable parameters.  In the future, capability to read a full age’ – age matrix could be created.  The dimension of the ageing error matrix requires the column length match the population maximum age specified at the top of the data file.  However, the maximum age for binning of age data may be lower that the population maximum age.  

\begin{center}
	\begin{tabular}{p{2cm} p{2cm} p{2cm} p{2cm} p{3cm} p{3cm} }
		\hline
		\multicolumn{1}{l}{2} & \multicolumn{5}{l}{\# Number of ageing error matrices to generate}\\
		\hline
		\#Age-0 & Age-1 & Age-2  &  ... & Max Age & \\
		\hline
		-1 & -1 & -1  & ... & -1  & \#Mean Age\\
		0.001 & 0.001 & 0.001 & ... & 0.001 & \#SD\\
		\hline
		0.5 & 1.5 & 2.3 & ... & Max Age + 0.5 & \#Mean Age\\
		0.5 & 0.65 & 0.67 & ... & 4.3 & \#SD Age \\
		\hline
	\end{tabular}
\end{center}
The above table shows the values for the first 3 ages for each of two age transition definitions: the first defines a matrix with no bias and negligible imprecision and the second shows a small negative bias beginning at age 2.

\begin{description}
	\item[Note:]\
	\begin{itemize}		
		\item In principle, one could have year or laboratory specific matrices.
		\item For each matrix, enter a vector with mean age’ for each true age; if there is no ageing bias, then set age’ equal to true age + 0.5.  Alternatively, -1 value for mean age’ means to set it equal to true age plus 0.5.  The addition of +0.5 is needed so that fish will get assigned to the intended interger age’.
		\item The length of the input vector is Nage+1, with the first entry being for age 0 fish and the last for fish of age Nage. The following line is a a vector with the standard deviation (stddev) of age’ for each true age.
		\item SS is able to create one ageing error matrix from parameters, rather than from an input vector.  The range of conditions in which this new feature will perform well has not been evaluated, so it should be considered as a preliminary implementation and subject to modification.
			\begin{itemize}
				\item To invoke this option, for the selected ageing error vector, set the stddev of ageing error to a negative value for age 0.  This will cause creation of an ageing error matrix from parameters and any age or size-at-age data that specify use of this age error pattern will use this matrix. Then in the control file, add 7 parameters below the cohort growth dev parameter.  These parameters are described in the control file section of this manual.
			\end{itemize}			  
	\end{itemize}
\end{description}


\begin{tabular}{p{2cm} p{2cm} p{2cm} p{1.5cm} p{1.5cm} p{2cm} p{2cm}}
		\multicolumn{7}{l}{Specify bin compression and error structure for age composition data for each fleet:}\\
		\hline
		\#Min Tail Compression & Constant added to proportions & Combine males \& females & Compress Bins & Error Dist. & Dirichlet Parameter Number & Minimum Sample Size\\
		\hline
		0 & 0.0001 & 1 & 0 & 0 & 0 & 1 \\
		0 & 0.0001 & 1 & 0 & 0 & 0 & 1 \\
		0 & 0.0001 & 1 & 0 & 0 & 0 & 1 \\
		\hline
\end{tabular}

			
\begin{tabular}{p{1cm} p{14cm}}
	 & \\
	\multicolumn{2}{l}{Specify method by which length bin range for age obs will be interpreted:}\\
	\hline
	1 & Bin method for age data\\
	  & 1 = value refers to population bin index\\
	  & 2 = value refers to data bin index\\
	  & 3 = value is actual length (which must correspond to population length bin  \\
	  & boundary)\\
	 \hline
\end{tabular}


\begin{tabular}{p{1cm} p{1cm} p{1cm} p{1cm} p{1.5cm} p{1cm} p{1cm} p{1cm} p{1cm} p{2.1cm}}
	\multicolumn{10}{l}{ }\\
	\multicolumn{10}{l}{An example age composition observation:}\\
	\hline
	\#Year & Month & Fleet & Sex & Partition & Age Err & Lbin lo & Lbin hi & Nsamp & Data Vector \\
	\hline
	1987 & 1 & 1 & 3 & 0 & 2 & -1 & -1 & 79 & <enter data values>\\
	-9999 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
	\hline
\end{tabular}


\begin{description}
	\item[Note:]\
	\begin{itemize}		
		\item Syntax for Sex, Partition, and data vector are same as for length.
		\item Ageerr identifies which ageing error matrix to use to generate expected value for this observation.
		\item The data vector has female values then male values, just as for the length composition data.
		\item As with the length comp data, a negative value for year causes the observation to not be read into the working matrix, a negative value for fleet causes the observation to be included in expected values calculation, but not in contribution to total logL, a negative value for month causes start-stop of super-period.
		\item Lbin lo, and Lbin hi are the range of length bins that this age composition observation refers to.  Normally these are entered with a value of -1 and -1 to select the full size range.  Whether these are entered as population bin number, length data bin number, or actual length is controlled by the value of the length bin range method above.
		\begin{itemize}
			\item Entering value of 0 or -1 for Lbin lo converts Lbin lo to 1;
			\item Entering value of 0 or -1 for Lbin hi converts Lbin hi to Maxbin;
			\item It is strongly advised to use the “-1” codes to select the full size range.  If you use explicit values, then the model could unintentionally exclude information from some size range if the population bin structure is changed.
			\item In reporting to the comp\_report.sso, the reported Lbin\_lo and Lbin\_hi values are always converted to actual length.
		\end{itemize}			  
	\end{itemize}
\end{description}

\subsection{Conditional Age-at-Length}
Use of conditional age’-at-length will greatly increase the total number of age’ composition observations and associated model run time, but it is a superior approach for several reasons.  First, it avoids double use of fish for both age’ and size information because the age’ information is considered conditional on the length information.  Second, it contains more detailed information about the relationship between size and age so provides stronger ability to estimate growth parameters, especially the variance of size-at-age.  Lastly, where age data are collected in a length-stratified program, the conditional age’-at-length approach can directly match the protocols of the sampling program.

In a two sex model, it is best to enter these conditional age’-at-length data as single sex observations (sex =1 for females and = 2 for males), rather than as joint sex observations (sex = 3).  Inputting joint sex observations comes with a more rigid assumption about sex ratios within each length bin. Using separate vectors with sex = 1 and 2 allows 100\% of the expected comp to be fit to 100\% observations within each sex, whereas with sex=3, you would have a bad fit if the sex ratio were out of balance with the model expectation, even if the observed proportion at age within each sex exactly matched the model expectation for that age.  Additionally, inputting the conditional age-at-length data as single sex observations isolates the age composition data from any sex selectivity as well.

When Lbin\textunderscore lo and Lbin\textunderscore hi are used to select a subset of the total size range, the expected value for these age’ data is calculated within that specified size range, so is age’ conditional on length.


\subsection{Mean Length or Body Weight-at-Age}
SS also accepts input of mean length-at-age’ or mean bodywt-at-age’.  This is done in terms of age’, not true age, to take into account the effects of ageing imprecision on expected mean size-at-age’.  If the value of “AgeErr” is positive, then the observation is interpreted as mean length-at-age’.  If the value of “AgeErr” is negative, then the observation is interpreted as mean bodywt-at-age’ and the abs(AgeErr) is used as AgeErr.

\begin{center}
	\begin{tabular}{p{0.75cm} p{1cm} p{0.75cm} p{1cm} p{0.75cm} p{1cm} p{1cm} p{3.2cm} p{3.2cm} }
		\hline
		1 & \multicolumn{8}{l}{\#Use mean size-at-age obsevation (0 = none, 1 = read data matrix)} \\
		\multicolumn{9}{l}{An example observation:}\\
		\hline
		\#Yr & Month & Fleet & Sex & Part & AgeErr & Ignore & Data Vector (Female - Male) & Sample Size (Female - Male) \\
		\hline
		1989  & 7 & 1 & 3 & 0 & 1 & 999 & <Mean Size values> & <Sample Sizes> \\
		...   &   &   &   &   &   &   &  & \\
		-9999 & 7 & 1 & 3 & 0 & 1 & 999 &  & \\
		\hline
	\end{tabular}
\end{center}

\begin{description}
	\item[Note:]\
	\begin{itemize}
		%\item Nsamp value is ignored if positive, but a negative value will cause the entire observation to be ignore.
		\item Negatively valued mean size entries with be ignored in fitting.
		\item Nfish value of 0 will cause mean size value to be ignored in fitting.
		\item Negative value for year causes observation to not be included in the working matrix.
		\item Each sexes' data vector and N fish vector has length equal to the number of age' bins.
		\item The "Ignore" column is not used but still needs to have default values in that column.
		\item Where age data are being entered as conditional age’-at-length and growth parameters are being estimated, it may be useful to include a mean length-at-age vector with nil emphasis to provide another view on the model’s estimates.
		\item An experiment that may be of interest might be to take the body weight-at-age data an enter it to SS as empirical body wt-at-true age in the wtatage.ss file, and to contrast results to entering the same body weight-at-age data here and to attempt to estimate growth parameters, potentially time-varying, that match these body weight data.		
	\end{itemize}
\end{description}

\subsection{Environmental Data}
SS accepts input of time series of environmental data.  Parameters can be made to be time-varying by making them a function of one of these environmental time series.

\begin{center}
	\begin{tabular}{p{1cm} p{3cm} p{3cm} p{8cm}}
		\multicolumn{4}{l}{\# Parameter values can be a function of an environmental data series: }\\
		\hline
		1 & \multicolumn{3}{l}{\#Number of environmental variables}\\
		\hline
		\multicolumn{4}{l}{COND > 0  Example of 2 environmental observations:} \\
		  & \#Year & Variable & Value \\
		\hline
		  & 1990 & 1 & 0.10 \\
		  & 1991 & 1 & 0.15 \\
		  & -9999 & 0 & 0 \\
		\hline
	\end{tabular}
\end{center}

\begin{description}
	\item[Note:]\
	\begin{itemize}
		\item Any years for which environmental data are not read are assigned a value of 0.0.
		\item It is permissible to include a year that is one year before the start year in order to assign environmental conditions for the initial equilibrium year.  But this works only for recruitment parameters, not biology or selectivity parameters.
		\item Environmental data can be read for up to 100 years after the end year of the model.  Then, if the recruitment-environment link has been activated, the future recruitments will be influenced by any future environmental data.  This could be used to create a future “regime shift” by setting historical values of the relevant environmental variable equal to zero and future values equal to 1, in which case the magnitude of the regime shift would be dictated by the value of the environmental linkage parameter.  Note that only future recruitment and growth can be modified by the environmental inputs; there are no options to allow environmentally-linked selectivity in the forecast years.
		\item Note that some model derived quantities like summary biomass and recruitment deviation are assigned to some negative valued environmental variables.  This is a stepping stone towards creating ability for parameters to be density-dependent.
	\end{itemize}
\end{description}

\subsection{Generalized Size Composition Data}
A flexible feature with SS is a generalized approach to size composition information.  It was designed initially to provide a means to include weight frequency data, but was implemented to provide a generalized capability.  The user can define as many size frequency methods as necessary.

\begin{itemize}
	\item Each method has a specified number of bins.
	\item Each method has "units" so the frequencies can be in units of biomass or numbers.
	\item Each method has “scale” so the bins can be in terms of weight or length (including ability to convert bin definitions in pounds or inches to kg or cm). 
	\item The composition data is input as females then males, just like all other composition data in SS.  So, in a two-sex model, the new composition data can be combined sex, single sex, or both sex.
	\item If a retention function has been defined, then the new composition data can be from the combined discard + retained, discard only or retained only.
\end{itemize}

\begin{center}
	\begin{tabular}{p{1.4cm} p{0.5cm} p{13 cm}}
		\multicolumn{3}{l}{Example entry:}\\
		\hline
		0 &  & \#N of size frequency methods\\
		\hline
		\multicolumn{3}{l}{COND > 0 }  \\
		\multicolumn{2}{r}{25 15} & \#Nbins per method\\
		%\hline
		\multicolumn{2}{r}{2 3} & \#Units per each method (1 = biomass, 2 = numbers)\\
		%\hline
		\multicolumn{2}{r}{3 3} & \#Scale per each method (1 = kg, 2 = lbs, 3 = cm, 4 = inches)\\
		%\hline
		\multicolumn{2}{r}{1e-9 1e-9} & \#Min compression to add to each observation (entry for each method)\\
		%\hline
		\multicolumn{2}{r}{2 2} & \#N observations per weight frequency method \\
		\hline
	\end{tabular}
\end{center}

\begin{center}
	\begin{tabular}{p{0.4cm} p{0.4cm} p{0.4cm} p{0.4cm} p{0.4cm} p{0.4cm} p{0.5cm} p{0.5cm} p{0.5cm} p{0.5cm} p{0.5cm} p{0.5cm} p{0.5cm} p{0.5cm} p{0.5cm} p{0.5cm} p{0.5cm} p{0.5cm} p{0.5cm} p{0.25cm}}
		\multicolumn{18}{l}{Then enter the lower edge of the bins for each method. The two row vectors shown}\\
		\multicolumn{18}{l}{below contain the bin definitions for methods 1 and 2 respectively:}\\
		\hline
		-26 & 28 & 30 & 32 & 34 & 36 & 38 & 40 & 42 & ... & 60 & 62 & 64 & 68 & 72 & 76 & 80 & 90\\
		-26 & 28 & 30 & 32 & 34 & 36 & 38 & 40 & 42 &  44 & 46 & 48 & 50 & 52 & \multicolumn{4}{l}{54} \\
		\hline 
	\end{tabular}
\end{center}

\begin{description}
	\item[Note:]\
	\begin{itemize}
		\item There is no tail compression for generalized size frequency data.
		\item Super-period capability is enabled in same way as for length and age composition data.
		\item There are two options for treating fish that in population size bins that are smaller than the smallest size frequency bin.
		\begin{itemize}
			\item Option 1:  By default, these fish are excluded (unlike length composition data where the small fish are automatically accumulated up into the first bin.
			\item Option 2:  If the first size bin is given a negative value, then:  accumulation is turned on and the negative of the entered value is used as the lower edge of the first size bin;
		\end{itemize}
		\item By choosing units=2 and scale=3, the size comp method can be nearly identical to the length comp method if the bins are set identically;
		\item Bin boundaries can be real numbers so obviously do not have to align with population length bin boundaries, SS interpolates as necessary;
		\item Size bins cannot be defined to be narrower than the population binwidth; an untrapped error will occur;
		\item Because the transition matrix can depend upon weight-at-length, it is calculated internally for each sex and for each season because weight-at-length can differ between sexes and can vary seasonally.
	\end{itemize}
\end{description}

An example observation is below.  Note that its format is identical to the length composition data, including sex and partition options, except for the addition of the first column to indicate the size frequency method.

\begin{center}
	\begin{tabular}{p{1.5cm} p{1cm} p{1cm} p{1cm} p{1cm} p{1cm} p{1cm} p{6cm}}
		\hline
		\#Method & Year & Month & Fleet & Sex & Part & Sample Size & <composition females then males>\\
		\hline
		1 & 1975 & 1 & 1 & 3 & 0 & 43 & <data> \\
		1 & 1977 & 1 & 1 & 3 & 0 & 43 & <data> \\
		1 & 1979 & 1 & 1 & 3 & 0 & 43 & <data> \\
		1 & 1980 & 1 & 1 & 3 & 0 & 43 & <data> \\
		\hline
	\end{tabular}
\end{center}

\subsection{Tag-Recapture Data}
An ability to analyze tag-recapture data is available with SS.  Each released tag group is characterized by an area, time, sex and age at release.  Each recapture event is characterized by a time and fleet.  Because SS fleet’s each operate in only one area, it is not necessary to record the area of recapture.  Inside the model, the tagged cohort is apportioned across all growth patterns in that area at that time (with options to apportion to only one sex or to both).  The tag cohort x growth pattern then behaves according to the movement and mortality of that growth pattern.  The number of tagged fish is modeled as a negligible fraction of the total population.  This means that a tagging event does not move fish from an untagged group to a tagged group.  Instead it acts as if the tags are seeded into the population with no impact at all on the total population abundance or mortality.  The choice to require assignment of a predominant age at release for each tag group is a pragmatic coding and model efficiency choice.  By assigning a tag group to a single age, rather than distributing it across all possible ages according to the size composition of the release group, it can be tracked as a single diagonal cohort through the age x time matrix with minimal overhead to the rest of the model.  Tags are considered to be released at the beginning of a season (period) and recaptures follow the timing of the fleet that made the recapture.

\begin{center}
	\begin{tabular}{p{1.1cm} p{1.1cm} p{1.1cm} p{1.1cm} p{1.1cm} p{1.1cm} p{1.1cm} p{1.1cm} p{3cm}}
		\multicolumn{9}{l}{Example set-up for tagging data:}\\
		\hline
		1 & & \multicolumn{7}{l}{\#Do tags - if this value is 0, then omit all entries below}\\
		\hline
		\multicolumn{9}{l}{COND = 1 All subsequent tag-recapture entries must be omitted if "Do Tags" = 0}\\
		%\hline
		 & 3 & \multicolumn{7}{l}{\#Number of tag groups}\\
		 \hline
		 & 12 & \multicolumn{7}{l}{\#Number of recapture events}\\
		 \hline
		 & 2 & \multicolumn{7}{l}{\#Mixing latency period: N periods to delay before comparing }\\
		 &   &  \multicolumn{7}{l}{observed to expected recoveries (0 = release period)}\\
		 \hline
		 & 10 & \multicolumn{7}{l}{\#Max periods (months) to track recoveries, after which tags enter}\\
		 &    & \multicolumn{7}{l}{ accumulator}\\
		 \hline
		 & \multicolumn{8}{l}{\#Release Data} \\
		 & \#TG & Area & Year & Month & <tfill> & Sex & Age & N Release\\ 
		 \hline
		 & 1 & 1 & 1980 & 1 & 999 & 0 & 24 & 2000 \\
		 & 2 & 1 & 1995 & 1 & 999 & 1 & 24 & 1000 \\
		 & 3 & 1 & 1985 & 1 & 999 & 2 & 24 & 10 \\
		 \hline
		 & \multicolumn{8}{l}{\#Recapture Data}\\
		 & \#TG &  & Year&  & Month &  & Fleet  & Number\\ 
		 \hline
		 & 1 & & 1982 & & 1 & & 1 & 7 \\
		 & 1 & & 1982 & & 1 & & 2 & 5 \\
		 & 1 & & 1985 & & 1 & & 2 & 0 \\
		 & 2 & & 1997 & & 1 & & 1 & 6 \\
		 & 2 & & 1997 & & 2 & & 1 & 4 \\
		 & 3 & & 1986 & & 1 & & 1 & 7 \\
		 & 3 & & 1986 & & 2 & & 1 & 5 \\
		 \hline
	\end{tabular}
\end{center}

\begin{description}
	\item[Note:]\
	\begin{itemize}
		\item The release data must be enter in TG order.
		\item <tfill> values are place holders and are replaced by program generated values for model time.
		\item Analysis of the tag-recapture data has one -logL component for the distribution of recaptures across areas and another -logL component for the decay of tag recaptures from a group over time, hence informative about mortality.  More on this in the control file.
	\end{itemize}
\end{description}

\subsection{Stock Composition Data}
It is sometimes possible to observe the fraction of a sample that is composed of fish from different stocks.  These data could come from genetics, otolith microchemistry, tags or other means.  The growth pattern feature in SS allows definition of cohorts of fish that have different biological characteristics and which are independently tracked as they move among areas.  SS now incorporates the capability to calculate the expected proportion of a sample of fish that come from different growth patterns.  In the inaugural application of this feature, there was a 3 area model with one stock spawning and recruiting in area 1, the other stock in area 3, then seasonally the stocks would move into area 2 where stock composition observations were collected, then they moved back to their natal area later in the year.

\begin{center}
	\begin{tabular}{p{1.1cm} p{1.1cm} p{1.1cm} p{1.1cm} p{1.1cm} p{1.1cm} p{1.1cm} p{1.1cm} p{3.5cm}}
		\multicolumn{9}{l}{Stock composition data can be entered in SS as follows:}\\
		\hline
		1 &  \multicolumn{8}{l}{\#Do morphcomp (if zero, then do not enter any further input below)}\\
		\hline
		\multicolumn{9}{l}{COND = 1}\\ 
		& 3 & \multicolumn{7}{l}{\#Number of observations}\\
		\hline
		& 2 & \multicolumn{7}{l}{\#Number of stocks}\\
		\hline
		& 0.0001 & \multicolumn{7}{l}{\#Minimum Compression}\\
		\hline
		& \#Year & Month & Fleet & Part & Nsamp & \multicolumn{3}{l}{Data Vector} \\
		\hline
		& 1980 & 1 & 1 & 0 & 36 & 0.4 & 0.6 & ...\\
		& 1981 & 1 & 1 & 0 & 40 & 0.44 & 0.62 & ...\\
		& 1982 & 1 & 1 & 0 & 50 & 0.49 & 0.50 & ...\\
		\hline
	\end{tabular}
\end{center}

\begin{description}
	\item[Note:]\
	\begin{itemize}
		\item The N stocks entered with these data must match the N growth patterns in the control file.
		\item The expected value is combined across sexes.
		\item The “partition” flag is included here in the data, but cannot be used because the expected value is calculated before the catch is partitioned into discard and retained components.
		\item Note that there is a specific value of mincomp to add to all values of observed and expected.
	\end{itemize}
\end{description}

\subsection{Selectivity Empirical Data}
It is sometimes possible to conduct field experiments or other studies to provide direct information about the selectivity of a particular length or age relative to the length or age that has peak selectivity, or to have a prior for selectivity that is more easily stated than a prior on a highly transformed selectivity parameter.  This section provides a way to input data that would be compared to the specified derived value for selectivity.  This is a placeholder at this time and will be fully implemented soon.

\begin{center}
	\begin{tabular}{p{1.5cm} p{1.5cm} p{1.5cm} p{1.5cm} p{1.5cm} p{1.5cm} p{1.5cm} p{1.5cm} p{1.1cm}}
		\multicolumn{9}{l}{Selectivity data can be entered in SS as follows:}\\
		\hline
		0 &  \multicolumn{8}{l}{\#Do data read for selectivity (if zero, then do not enter any further input below)}\\
        \hline
		& \#Year & Month & Fleet & Age/Size & Bin\# & \_datum & datum\_se \\
		\hline
	\end{tabular}
\end{center}

\begin{center}
	\begin{tabular}{p{2cm} p{14cm}}\\
		\multicolumn{2}{l}{End of Data File}\\
		\hline
		999 & \#End of data file marker\\
		\hline
	\end{tabular}
\end{center}


\subsection{Excluding Data}
Data that are before the model start year or greater than the retrospective year are not moved into the internal working arrays at all.  So if you have any alternative observations that are used in some model runs and not in others, you can simply give them a negative year value rather than having to comment them out. The first output to data.ss\textunderscore new has the unaltered and complete input data.  Subsequent reports to data.ss\textunderscore new produce expected values or bootstraps only for the data that are being used.

Data that are to be included in the calculations of expected values, but excluded from the calculation of negative log likelihood, are flagged by use of a negative value for fleet ID.

\subsection{Data Super Periods}
The “Super-Period” capability allows the user to introduce data that represent a blend across a set of time steps and to cause the model to create an expected value for this observation that uses the same set of time steps.  The option is available for all types of data and a similar syntax is used.  Super-periods are started with a negative value for month, and then stopped with a negative value for month, placeholder observations within the super-period are designated with a negative fleet field.  The standard error (se) or Nsamp field is now used for weighting of the expected values.  An error message is generated if the super-period does not contain exactly one observation with a positive fleet field.

All super-period observations must be contiguous in the data file.  All but one of the observations in the sequence will have a negative value for fleet ID so the data associated with these dummy observations will be ignored. The observed values must be combined outside of the model and then inserted into the data file for the one observation with a positive fleet ID.
An expected value for the observation will be computed for each selected time period within in the super-period.  The expected values are weighted according to the values entered in the se (or Nsamp) field for all observations expect the single observation holding the combined data.  The expected value for that year gets a relative weight of 1.0.  So in the example below, the relative weights are:  1982, 1.0 (fixed); 1983, 0.85; 1985, 0.4; 1986, 0.4.  These weights are summed and rescaled to sum to 1.0, and are output in the echoinput.sso file.

Not all time steps within the extent of a super-period need be included.  For example, in a 3 season model a super-period could be set up to combine information from season 2 across 3 years, e.g. skip over the season 1 and season 2 for the purposes of calculating the expected value for the super-period.  The key is to create a dummy observation (negative fleet value) for all time steps, except 1, that will be included in the super-period and to include one real observation (positive fleet value; which contains the real combined data from all the specified time steps).

\begin{center}
	\begin{tabular}{p{1cm} p{1cm} p{1cm} p{1cm} p{1cm} p{9cm}}
		\multicolumn{6}{l}{Example:}\\
		\hline
		\#Year & Month & Fleet & Obs & SE & Comment \\
		\hline
		1982 & \textbf{-2} & 3 & 34.2 & 0.3 & Start super-period.  This observation has positive fleet value, so is expected to contain combined data from all identified periods of the super-period.  The se entered here is use as the se of the combined observation.  The expected value for the survey in 1982 will have a relative weight of 1.0 (default) in calculating the combined expected value.\\
		\hline
		1983 & 2 & \textbf{-3} & 55 & 0.3 & In super-period; entered obs is ignored.  The expected value for the survey in 1983 will have a relative weight equal to the value in the se field (0.85) in calculating the combined expected value.\\
		\hline
		1985 & 2 & \textbf{-3}& 88 & 0.40 & Note that 1984 is not included in the supe-rperiod.  Relative weight for 1985 is 0.4\\
		\hline
		1986 & \textbf{-2} & \textbf{-3} & 88 & 0.40 & End super-period\\
		\hline
	\end{tabular}
\end{center}

A time step that is within the time extent of the super-period can still have its own separate observation.  In the above example, the survey observation in 1984 could be entered as a separate observation, but it must not be entered inside of the contiguous block of super-period observations.  For composition data (which allow for replicate observations), a particular time steps' observations could be entered as a member of a super-period and as a separate observation.

The super-period concept can also be used to combine seasons within a year with multiple seasons.  This usage could be preferred if fish are growing rapidly within the year so their effective age selectivity is changing within year as they grow; fish are growing within the year so fishery data collected year round have a broader size-at-age modes than a mid-year model approximation can produce; and it could be useful in situations with very high fishing mortality.
